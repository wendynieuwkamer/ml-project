\documentclass{article}

\usepackage{booktabs} 
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}

\title {Project Report}
\date{21 December 2016}
\author{Elise Mol, Wendy Nieuwkamer, and Isobel Smith}

\begin{document}

\maketitle

\section{Problem}

After three months of learning about several machine learning techniques and classifying algorithms it was time to put our newly gained skills into practice. We decided to try our hand at one of the competitions hosted by Kaggle. The challenge we chose is Leaf Classification; the objective of the competition is to use binary leaf images and their extracted features to predict what species each leaf belongs to. The dataset which was provided includes around 1584 images of leaf specimens; these samples include 99 species. From every image three sets of features are extracted: a shape contiguous descriptor, an interior texture histogram and a fine-scale margin histogram. Every feature is represented as a 64-attribute vector. 

As the competition started at 30 August 2016, there were already quite some submissions made and kernels active. One of them stood out, namely "10 Classifier Showdown in Scikit-Learn". In this kernel Jeff Delaney compared ten classifiers which are part of the python library Scikit-Learn. The results of his experiment are shown in figure \ref{fig:showdown}. In his experiment Delaney used the default setup for the classifiers , combined with semi-random parameters. He notes that the performance of the classifiers could be improved by tuning the hyper-parameters. Thus, our research question is whether we can improve the accuracy found by Delaney by tuning the hyper-parameters of the classifiers.

\begin{figure} 
	\includegraphics[width=\linewidth]{results-showdown.png}
	\caption{The 10 Classifier Showdown}
	\label{fig:showdown}
\end{figure}

As we did not have time to run experiments for all ten classifiers, we decided to look into five of them. We chose the K-neigbors, decision tree and Gaussian Naive Bayes as they had relatively decent accuracy at respectively 88\%, 65\%, and 57\%. Also, we had some background in these algorithms as they had been taught in our Machine Learning course. Additionally, we decided to choose either Ada boost or quadratic discriminant analysis as they could improve the most with found accuracies of 4.5\% and 2.5\%. 

\newpage
\section{The algorithms}

\subsection{K-neighbors}


\section{Choices made and justifications}
Which classifiers you chose ; why?  
Describe the relevant features of the classifiers.

\newpage

\newpage

\section{Solution}
How do we optimize them? k-fold cv, gridsearch
\newpage

\section{Evaluation of Approach and Solution} 
All the curves, the results

\newpage

\section{Sources}


https://www.kaggle.com/jeffd23/leaf-classification/10-classifier-showdown-in-scikit-learn



















\end{document}